## 제품 요구사항 정의서: 설정 기반 웹 콘텐츠 추출기

**버전:** 1.0
**날짜:** 2023-10-27
**작성자:** (`webcrawler.py` 분석 기반)
**상태:** 정의

**1. 소개**

이 문서는 다수의 대상 웹사이트에서 특정 콘텐츠 섹션을 추출하고, 마크다운(Markdown) 형식으로 변환하여 로컬에 저장하도록 설계된 Python 기반 웹 크롤러의 요구사항을 정의합니다. 크롤러의 동작은 외부 CSV 파일을 통해 구성되므로, 사용자는 핵심 스크립트를 수정하지 않고도 대상 URL, 반복을 위한 관련 파라미터, 원하는 콘텐츠 블록을 식별하기 위한 마커를 지정할 수 있습니다. 주요 사용 사례는 초기 정리 및 마크다운 변환 후 HTML 구조 내에서 시작 및 종료 마커로 식별된 콘텐츠에 특히 초점을 맞춰 특정 웹 페이지(예: 공지사항 또는 게시물)에서 업데이트나 기사를 수집하는 것입니다.

**2. 목표 및 목적**

*   **콘텐츠 수집 자동화:** 미리 정의된 웹 페이지에서 특정 콘텐츠 섹션을 자동으로 가져와 저장합니다.
*   **설정 가능성:** 사용자가 핵심 스크립트를 수정하지 않고 CSV 파일을 통해 대상 웹사이트 및 추출 파라미터를 쉽게 정의하고 관리할 수 있도록 합니다.
*   **콘텐츠 분리:** 사용자가 정의한 시작 및 종료 텍스트 마커를 사용하여 웹 페이지 콘텐츠의 관련 부분만 추출합니다.
*   **형식 변환:** 추출된 HTML 콘텐츠를 깔끔한 마크다운 형식으로 변환합니다.
*   **구조화된 저장:** 추출된 콘텐츠를 사이트 이름과 크롤링 날짜를 기준으로 계층적 디렉토리 구조에 저장합니다.
*   **기본적인 서버 부하 방지:** 대상 서버에 과부하를 주지 않도록 요청 사이에 간단한 지연을 구현합니다.

**3. 범위**

**3.1. 포함 범위:**

*   지정된 CSV 파일(`server.csv`)에서 크롤러 설정 읽기.
*   초기 숫자 파라미터 값을 추출하기 위해 기본 URL 파싱.
*   추출된 숫자 파라미터 값을 (현재 10회 반복으로) 감소시키면서 대상 URL 시퀀스 생성.
*   HTTP GET 요청을 사용하여 생성된 URL에서 HTML 콘텐츠 가져오기.
*   표준 브라우저 `User-Agent` 헤더 사용.
*   HTTP 요청에 대한 타임아웃 설정.
*   HTTP 요청 실패(예: 연결 오류, 타임아웃, 2xx 외 상태 코드)에 대한 기본 오류 처리.
*   BeautifulSoup를 사용하여 가져온 HTML 콘텐츠 파싱.
*   파싱된 HTML에서 `<script>` 및 `<style>` 요소 제거.
*   정리된 HTML 스니펫을 마크다운 형식으로 변환 (`markdownify` 라이브러리, ATX 헤딩 스타일 사용).
*   생성된 마크다운 콘텐츠에서 지정된 시작 및 종료 마커 문자열 사이의 텍스트 줄 추출 (시작 포함, 종료 제외).
*   디렉토리 구조 생성: `./<사이트명>/<YYYY-MM-DD>/`.
*   추출된 마크다운 콘텐츠를 날짜별 디렉토리 내에 `<숫자_파라미터_값>.md`라는 이름의 파일로 저장.
*   각 URL 처리 후 무작위 지연(1~3초) 구현.
*   상태 메시지(처리 중인 사이트, 크롤링 URL, 파일 저장, 오류)를 표준 출력으로 인쇄.

**3.2. 제외 범위:**

*   콘텐츠 렌더링을 위해 JavaScript 실행이 필요한 웹사이트 크롤링.
*   복잡한 로그인 메커니즘 또는 세션 관리 처리.
*   고급 안티봇 조치 우회 기술(예: CAPTCHA 해결, 복잡한 헤더 조작, IP 순환).
*   추출된 데이터를 개별 마크다운 파일 이외의 데이터베이스 또는 다른 형식으로 저장.
*   그래픽 사용자 인터페이스(GUI) 제공.
*   증분 크롤링 또는 변경 사항 감지 (실행할 때마다 콘텐츠를 다시 가져옴).
*   기본 요청 예외를 넘어서는 고급 오류 처리 및 재시도 로직.
*   단순 지연 외의 정교한 속도 제한 또는 `robots.txt` 준수.
*   기본 행 길이 확인 이상의 CSV 파일 내용에 대한 입력 유효성 검사.
*   반복을 위한 숫자가 아닌 파라미터 값 처리.

**4. 기능 요구사항**

| ID    | 요구사항 설명                                                                                                                                                                                               | 세부사항 및 제약조건                                                                                                                                                                                                 | 우선순위 |
| :---- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------- |
| FR1   | **설정 로딩:** 시스템은 스크립트와 동일한 디렉토리에 있는 `server.csv`라는 CSV 파일에서 크롤러 설정을 읽어야 합니다.                                                              | - CSV 형식: 행당 `url, param_name, site_name, category, start_marker, end_marker`.<br>- 인코딩: UTF-8.<br>- 6개 미만의 열이 있는 행은 건너뛰거나 적절히 처리해야 합니다 (현재는 암묵적으로 건너뜀). | 필수     |
| FR2   | **초기 파라미터 추출:** 각 설정 행에 대해, 시스템은 제공된 `url`에서 `param_name`과 연관된 초기 *정수(integer)* 값을 추출해야 합니다.                                          | - URL에 `param_name=`이 존재한다고 가정합니다.<br>- `param_name=` 바로 뒤에 오는 연속된 숫자를 추출합니다.<br>- 추출에 실패하거나 숫자가 아니면 이 URL에 대한 처리를 건너뛰어야 합니다.          | 필수     |
| FR3   | **URL 생성:** 시스템은 초기 파라미터 값을 0, 1, 2, ..., 9만큼 감소시키고 URL을 재구성하여 각 설정에 대해 10개의 대상 URL을 생성해야 합니다.                                    | - `param_name`과 연관된 원래 숫자 값을 새로운 감소된 값으로 대체하여 URL을 재구성합니다.<br>- 숫자 값 뒤에 오는 URL의 잠재적인 비-파라미터 부분을 처리합니다.         | 필수     |
| FR4   | **웹 페이지 가져오기:** 시스템은 HTTP GET 요청을 사용하여 각 생성된 URL의 HTML 콘텐츠를 가져와야 합니다.                                                                                                       | - `User-Agent` 헤더 포함: 'Mozilla/5.0...'.<br>- 요청 타임아웃을 10초로 설정합니다.<br>- 성공적이지 않은 HTTP 상태 코드(예: 4xx, 5xx)에 대해 오류를 발생시킵니다.                                                  | 필수     |
| FR5   | **HTML 파싱 및 정리:** 시스템은 가져온 HTML 콘텐츠를 파싱하고 `<script>` 및 `<style>` 태그와 그 내용을 제거해야 합니다.                                                                         | - 표준 파서(예: `html.parser`)와 함께 `BeautifulSoup`를 사용합니다.                                                                                                                                                 | 필수     |
| FR6   | **마크다운 변환:** 시스템은 정리된 HTML 구조를 마크다운 형식으로 변환해야 합니다.                                                                                                                    | - `markdownify` 라이브러리를 사용합니다.<br>- 링크를 보존합니다.<br>- 헤더에 ATX 스타일을 사용합니다 (`# 헤더`).                                                                                                                  | 필수     |
| FR7   | **콘텐츠 섹션 추출:** 시스템은 생성된 마크다운 텍스트를 줄 단위로 처리하고 설정에 지정된 `start_marker`와 `end_marker` 사이의 줄만 추출해야 합니다.           | - `start_marker`가 포함된 줄 이후부터 검색을 시작합니다.<br>- `end_marker`가 포함된 줄 *이전*에서 추출을 중지합니다.<br>- 마커는 단순 문자열 포함 여부 확인입니다.<br>- 시작 마커를 찾지 못하면 콘텐츠가 추출되지 않습니다.<br>- 시작 마커 이후 종료 마커를 찾지 못하면 끝까지의 콘텐츠가 추출됩니다. | 필수     |
| FR8   | **출력 파일 저장:** 시스템은 추출된 마크다운 콘텐츠를 로컬 파일에 저장해야 합니다.                                                                                                                        | - 디렉토리 구조: `./<site_name>/<YYYY-MM-DD>/`. 디렉토리가 없으면 생성해야 합니다.<br>- 파일명: `<current_parameter_value>.md`.<br>- 파일 인코딩: UTF-8.                                       | 필수     |
| FR9   | **요청 제한(스로틀링):** 시스템은 각 URL 처리 후 1초에서 3초 사이의 무작위 시간 동안 실행을 일시 중지해야 합니다.                                                                                   | - `time.sleep()` 및 `random.uniform(1, 3)`를 사용합니다.                                                                                                                                                                   | 필수     |
| FR10  | **기본 로깅:** 시스템은 실행 중 상태 정보를 콘솔에 출력해야 합니다.                                                                                                                         | - 처리 중인 사이트/카테고리를 표시합니다.<br>- 크롤링 중인 URL을 출력합니다.<br>- 성공적인 저장 및 출력 파일 경로를 확인합니다.<br>- 크롤링 또는 처리 중 발생한 오류 메시지를 출력합니다.     | 필수     |
| FR11  | **오류 처리 (네트워크):** 시스템은 웹 페이지 가져오기 중 예외를 잡아 오류 메시지를 출력해야 합니다.                                                                                                  | - `requests.RequestException`을 잡습니다.<br>- 실패한 URL과 오류 메시지를 출력합니다.<br>- 다음 URL 또는 설정 처리를 계속합니다.                                                                          | 필수     |
| FR12  | **오류 처리 (콘텐츠 처리):** 시스템은 콘텐츠 처리(파싱, 마크다운 변환, 마커 로직) 중 일반적인 예외를 잡아 오류 메시지를 출력해야 합니다.                                  | - 일반 `Exception`을 잡습니다.<br>- 처리 실패를 나타내는 오류 메시지를 출력합니다.<br>- 해당 URL에 대해 빈 콘텐츠를 반환/저장합니다.                                                                                | 권장   |

**5. 비기능 요구사항**

| ID   | 요구사항 설명      | 세부사항 및 제약조건                                                                                                                               | 우선순위 |
| :--- | :--------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------- | :------- |
| NFR1 | **신뢰성:**             | 스크립트는 일반적인 네트워크 오류를 정상적으로 처리하고 다른 URL/설정 처리를 계속해야 합니다. 단일 실패로 인해 충돌하지 않아야 합니다. | 필수     |
| NFR2 | **유지보수성:**         | 코드는 명확한 작업을 위한 함수들로 합리적으로 잘 구조화되어야 합니다. 명확성을 위해 타입 힌트를 사용해야 합니다.                              | 권장   |
| NFR3 | **사용성 (개발자):**   | 외부 `server.csv` 파일을 통한 설정은 간단해야 합니다. 콘솔 출력은 프로세스에 대한 적절한 피드백을 제공해야 합니다.         | 필수     |
| NFR4 | **성능:**             | 속도에 대해 고도로 최적화되지는 않았지만(사이트당 순차적 요청), 스크립트는 사이트당 10페이지에 대해 합리적인 시간 내에 완료되어야 합니다. | 권장   |
| NFR5 | **리소스 사용량:**      | 메모리 및 CPU 사용량은 일반적인 웹 페이지 처리에 합리적이어야 합니다.                                                                         | 권장   |

**6. 입력/출력 명세**

**6.1. 입력:**

*   **`server.csv` 파일:**
    *   위치: 스크립트와 동일한 디렉토리.
    *   인코딩: UTF-8.
    *   컬럼:
        1.  `url`: 감소시킬 파라미터를 포함하는 기본 URL (예: `http://example.com/notice?boardId=123&page=1`).
        2.  `param_name`: 값이 감소될 쿼리 파라미터의 이름 (예: `boardId`).
        3.  `site_name`: 웹사이트의 사람이 읽을 수 있는 이름, 출력 디렉토리 생성에 사용됨 (예: `Example Notices`).
        4.  `category`: 카테고리 이름, 현재 로깅 컨텍스트에 사용됨 (예: `Government`).
        5.  `start_marker`: 추출할 콘텐츠 블록의 시작을 식별하는 문자열 (마크다운 출력에서 검색됨).
        6.  `end_marker`: 추출할 콘텐츠 블록의 끝을 식별하는 문자열 (마크다운 출력에서 검색됨).

**6.2. 출력:**

*   **마크다운 파일:**
    *   위치: `./<site_name>/<YYYY-MM-DD>/<numeric_parameter_value>.md`
    *   내용: 추출되고 정리된 콘텐츠 (마크다운 형식).
    *   인코딩: UTF-8.
*   **콘솔 출력:**
    *   진행 상황 및 오류를 나타내는 상태 메시지 (FR10에 정의됨).

**7. 오류 처리 전략**

*   **네트워크 오류 (`requests.RequestException`):** URL과 함께 오류를 기록하고 다음 URL로 계속 진행합니다. 재시도는 시도하지 않습니다.
*   **파라미터 추출 오류:** URL과 함께 오류를 기록하고 이 전체 설정 항목(사이트) 처리를 건너뜁니다.
*   **콘텐츠 처리 오류 (`process_content` 중 `Exception`):** 오류를 기록하고 콘텐츠에 대해 빈 문자열을 반환하여 빈 `.md` 파일이 저장되도록 합니다(또는 빈 콘텐츠가 저장을 방지해야 하는 경우 구현 세부 사항에 따라 파일 없음).
*   **파일 시스템 오류 (디렉토리/파일 생성):** 권한이 잘못된 경우 표준 Python `IOError`/`OSError` 예외가 스크립트를 중단시킬 수 있습니다. 기본 처리는 `os.makedirs(exist_ok=True)`를 포함합니다. 더 강력한 처리는 이 범위에 포함되지 않습니다.

**8. 의존성**

시스템은 다음 Python 라이브러리가 설치되어 있어야 합니다:

*   `requests`
*   `beautifulsoup4`
*   `markdownify`
*   `typing` (표준 라이브러리지만, 사용은 Python 3.5+를 의미함)

**9. 향후 고려사항 / 잠재적 개선사항 (v1.0 범위 외)**

*   숫자가 아니거나 더 복잡한 페이지네이션/반복 파라미터 지원.
*   사이트당 페이지/반복 횟수를 구성하는 기능.
*   지수 백오프를 포함한 더 강력한 오류 처리 및 재시도.
*   상당히 향상된 성능을 위한 비동기 가져오기 (예: `asyncio`와 `aiohttp` 사용, 또는 Scrapy로 마이그레이션).
*   JavaScript 렌더링 지원 (예: `requests-html`, `Selenium`, 또는 `Playwright` 사용).
*   결과 저장을 위한 데이터베이스 통합.
*   새롭거나 변경된 콘텐츠만 다운로드하기 위한 델타 체킹.
*   더 정교한 콘텐츠 추출 방법 (예: 마크다운 변환 전 HTML에 직접 CSS 선택자, XPath 사용).
*   `robots.txt` 존중.
*   더 나은 설정 관리 (예: YAML 또는 JSON 파일).
*   모니터링 및 알림 기능.

---
